{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.8 64-bit ('base': conda)",
   "display_name": "Python 3.7.8 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1f9f2f50a2b28fa060fdddcb5a436d7ea76e913cddb9e91a54e91d5e4221a97a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Import repo and funcs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import torch"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "source": [
    "Import function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.readlines()[3:]\n",
    "        data = [float(data[i].strip()) for i in range(len(data))]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(din, dout, d1, d2, l1, l2):\n",
    "    data = []\n",
    "    for i in range(d1):\n",
    "        data.append(get_data(r'D:\\\\My Files\\\\核技术专题研讨\\\\n-y data\\\\n-y data\\\\neutron\\\\' + str(i+1) + '.csv'))\n",
    "    for i in range(d2):\n",
    "        data.append(get_data(r'D:\\\\My Files\\\\核技术专题研讨\\\\n-y data\\\\n-y data\\\\gamma\\\\' + str(i+1) + '.csv'))\n",
    "    data1 = data[:d1]\n",
    "    data2 = data[d1:]\n",
    "\n",
    "    indata = torch.empty(l1+l2, din)\n",
    "    for i in range(l1):\n",
    "        for j in range(din):\n",
    "            indata[i, j] = data1[i][j]\n",
    "    for i in range(l2):\n",
    "        for j in range(din):\n",
    "            indata[i+l1, j] = data2[i][j]\n",
    "    \n",
    "    outdata = torch.empty(l1+l2, dout)\n",
    "    for i in range(l1):\n",
    "        outdata[i, 0] = 0\n",
    "        outdata[i, 1] = 1\n",
    "    for i in range(l2):\n",
    "        outdata[i+l1, 0] = 1\n",
    "        outdata[i+l1, 1] = 0\n",
    "\n",
    "    checkdata = torch.tensor(data1[l1:] + data2[l2:])\n",
    "    label = torch.tensor([[0, 1] for i in range(len(data1[l1:]))] + [[1, 0] for i in range(len(data2[l2:]))])\n",
    "\n",
    "    return [indata, outdata, checkdata, label]"
   ]
  },
  {
   "source": [
    "Check model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(model, din, dout):\n",
    "    checkdata = read_data(din, dout, 33, 28, 20, 20)[2]\n",
    "    label = read_data(din, dout, 33, 28, 20, 20)[3]\n",
    "    lossfunc = torch.nn.MSELoss()\n",
    "    loss = lossfunc(model(checkdata), label)\n",
    "    print('loss = ', loss.item())\n",
    "\n",
    "    result = []\n",
    "    correct = 0\n",
    "    for i in range(len(checkdata)):\n",
    "        if (model(checkdata)[i][0] > 0.5 and model(checkdata)[i][1] < 0.5) or (model(checkdata)[i][0] > model(checkdata)[i][1]):\n",
    "            result.append([1, 0])\n",
    "        elif (model(checkdata)[i][1] > 0.5 and model(checkdata)[i][0] < 0.5) or (model(checkdata)[i][1] > model(checkdata)[i][0]):\n",
    "            result.append([0, 1])\n",
    "        print(i+1, model(checkdata)[i], result[i], list(label[i]))\n",
    "        if result[i] == list(label[i]):\n",
    "            correct += 1\n",
    "    corate = correct / len(checkdata)\n",
    "    with open('accu.txt', 'a') as f:\n",
    "        print('Accuracy = ', 100*corate, '%', file=f)"
   ]
  },
  {
   "source": [
    "Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, din, H, dout, it = 40, 252, 100, 2, 20000\n",
    "indata = read_data(din, dout, 33, 28, 20, 20)[0]\n",
    "outdata = read_data(din, dout, 33, 28, 20, 20)[1]"
   ]
  },
  {
   "source": [
    "Define model, loss function and optimizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(din, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, dout),\n",
    ")\n",
    "\n",
    "lossfunc = torch.nn.MSELoss()\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "source": [
    "## Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " 1.7881096333016407e-15\n14358 4.334502219226644e-15\n14359 4.074095492120466e-15\n14360 2.963395587946005e-15\n14361 3.1166149542583618e-15\n14362 3.67660834096244e-15\n14363 2.583688928900268e-15\n14364 3.838501324654681e-15\n14365 3.0273878683198833e-15\n14366 2.3045286805836425e-15\n14367 3.903553455003811e-15\n14368 4.701829915264733e-15\n14369 2.640119110879297e-15\n14370 4.876264492290495e-15\n14371 6.005655872512016e-15\n14372 3.652907512065318e-15\n14373 4.408366033326061e-15\n14374 4.909625308434579e-15\n14375 1.739615938280148e-15\n14376 6.030085996776725e-15\n14377 6.786368257578673e-15\n14378 2.111505330255675e-15\n14379 6.67330757032859e-15\n14380 7.009250578088539e-15\n14381 2.1946528372152356e-15\n14382 4.6842844747953076e-15\n14383 4.1994106285349504e-15\n14384 2.020532721171142e-15\n14385 3.624653034043756e-15\n14386 2.7625606880202214e-15\n14387 3.0704957043462596e-15\n14388 5.70615603379121e-15\n14389 5.749949754746625e-15\n14390 2.7487697210895005e-15\n14391 4.752114873211432e-15\n14392 4.583066155180787e-15\n14393 3.974218872694396e-15\n14394 2.67944579182913e-15\n14395 5.2080409449829e-15\n14396 6.077917947308175e-15\n14397 3.821913031415653e-15\n14398 4.714146197834284e-15\n14399 4.2917846536307154e-15\n14400 2.829434362722422e-15\n14401 3.556627818049763e-15\n14402 2.996485353546968e-15\n14403 4.3824887532709685e-15\n14404 7.07784542373956e-15\n14405 4.126310838153957e-15\n14406 4.111771517614337e-15\n14407 7.114461811016416e-15\n14408 6.921037178829687e-15\n14409 3.975986206938842e-15\n14410 5.6613704372545596e-15\n14411 7.444492952321004e-15\n14412 8.372816498556247e-15\n14413 2.8763150952541058e-15\n14414 1.0849968587966184e-14\n14415 3.4395149251800154e-15\n14416 6.279842555120601e-15\n14417 5.239463749743666e-15\n14418 7.625020237336366e-15\n14419 4.73381091473774e-15\n14420 6.042605567253617e-15\n14421 5.597402509077915e-15\n14422 9.343429011716108e-15\n14423 7.128837654197216e-15\n14424 2.055409641587517e-14\n14425 5.440670073616821e-15\n14426 1.1179569436533567e-14\n14427 4.628017346658625e-15\n14428 7.22351052671183e-15\n14429 7.364522030671884e-15\n14430 5.542606677170613e-15\n14431 7.093493086890662e-15\n14432 5.203750299588583e-15\n14433 1.3823867384458143e-14\n14434 4.197044018480322e-15\n14435 8.105443772491849e-15\n14436 4.075334277805825e-15\n14437 9.520458050659309e-15\n14438 4.273092754067181e-15\n14439 1.1463887903740734e-14\n14440 5.97087500563186e-15\n14441 7.589797219257743e-15\n14442 3.4650368749137347e-15\n14443 5.681143574375264e-15\n14444 4.79581669109207e-15\n14445 4.843532174593273e-15\n14446 3.962859313838768e-15\n14447 3.4848561753300644e-15\n14448 3.544137681967788e-15\n14449 4.623179094463909e-15\n14450 2.99750729879783e-15\n14451 3.4134072521632697e-15\n14452 4.13501706630231e-15\n14453 3.114536123647563e-15\n14454 3.786684507622873e-15\n14455 3.715509176098041e-15\n14456 4.069978488480336e-15\n14457 3.902620871728884e-15\n14458 3.868620545709623e-15\n14459 3.8740280040448945e-15\n14460 4.128113324265714e-15\n14461 5.4112962415554626e-15\n14462 4.7843156777342515e-15\n14463 8.89392726133309e-15\n14464 4.308004064021214e-15\n14465 8.987702278923614e-15\n14466 5.248983129521384e-15\n14467 6.763678786020572e-15\n14468 5.121047273168094e-15\n14469 1.1019137161158866e-14\n14470 4.655187198991228e-15\n14471 1.0369214709961272e-14\n14472 4.509622469423208e-15\n14473 2.18328264809612e-14\n14474 1.0736865125552265e-14\n14475 8.745253497332173e-15\n14476 2.9615632862745045e-14\n14477 1.1010542317843076e-14\n14478 9.294401897696081e-15\n14479 1.0449030624581045e-14\n14480 8.283402006578188e-15\n14481 1.3774365931987654e-14\n14482 4.4664276007615016e-15\n14483 8.591209544445665e-15\n14484 6.797635913359523e-15\n14485 7.009879500051875e-15\n14486 1.11755333245399e-14\n14487 5.068571888019796e-15\n14488 1.2864839740917879e-14\n14489 4.1673182442293795e-15\n14490 1.0109266227679558e-14\n14491 6.372737081962454e-15\n14492 8.214731351478387e-15\n14493 3.789243394156528e-15\n14494 3.716786078266027e-15\n14495 6.43318939989152e-15\n14496 4.443876195573803e-15\n14497 6.657014891588153e-15\n14498 5.320193612913526e-15\n14499 8.322259643033479e-15\n14500 3.4064033484806607e-15\n14501 7.76080554801186e-15\n14502 8.663209039028175e-15\n14503 3.5479567417687208e-15\n14504 4.166383119855611e-15\n14505 8.782224791413929e-15\n14506 5.707123768933448e-15\n14507 3.418099179416348e-15\n14508 5.484428642704925e-15\n14509 4.848346286348993e-15\n14510 4.199540648092354e-15\n14511 4.403248683775224e-15\n14512 5.374270737365083e-15\n14513 5.107763255456305e-15\n14514 2.921114456076149e-15\n14515 3.6710568870261355e-15\n14516 5.313859077017485e-15\n14517 2.2020146123180595e-15\n14518 4.191135116640276e-15\n14519 4.728162475528975e-15\n14520 3.0044110408344264e-15\n14521 2.814415409776419e-15\n14522 3.209238430557093e-15\n14523 2.168157646625121e-15\n14524 3.8051510964224374e-15\n14525 3.1211578037127234e-15\n14526 4.0170694224630436e-15\n14527 2.2448297984611594e-15\n14528 2.6635946290124498e-15\n14529 3.367271696866933e-15\n14530 2.2250104980448297e-15\n14531 2.2570840358673242e-15\n14532 4.5809845717129094e-15\n14533 3.7429394910619244e-15\n14534 3.234391920958757e-15\n14535 3.4310797475747834e-15\n14536 3.5064968083412277e-15\n14537 3.580504406283441e-15\n14538 5.0485577700255975e-15\n14539 4.99218730386935e-15\n14540 4.267116089591355e-15\n14541 3.566995501324156e-15\n14542 4.9981423690050215e-15\n14543 3.258504619626482e-15\n14544 4.84947622830063e-15\n14545 4.165626719433713e-15\n14546 8.749814769753138e-15\n14547 7.740110839044544e-15\n14548 1.3423604506331965e-14\n14549 5.178984326760288e-15\n14550 5.372048122911487e-15\n14551 4.6166116245073725e-15\n14552 4.0634161007214835e-15\n14553 7.669657179590773e-15\n14554 9.449529205722128e-15\n14555 1.2230916895060971e-14\n14556 6.37822331436182e-15\n14557 4.466688063392782e-15\n14558 6.468732172907731e-15\n14559 6.682623662198966e-15\n14560 2.904938032607723e-15\n14561 8.603404277787285e-15\n14562 3.449229334293838e-15\n14563 7.217330574328662e-15\n14564 5.173303276783054e-15\n14565 5.36620444260838e-15\n14566 3.2275099900205525e-15\n14567 4.909367810418614e-15\n14568 3.307415690132734e-15\n14569 3.047901100478251e-15\n14570 3.428781323672409e-15\n14571 3.033565067845972e-15\n14572 3.668194762697363e-15\n14573 2.8079644068501304e-15\n14574 4.6329695247847476e-15\n14575 3.5433569293486563e-15\n14576 4.424772637997903e-15\n14577 4.202204990227942e-15\n14578 5.630402912702942e-15\n14579 4.017199442020447e-15\n14580 9.009213527652084e-15\n14581 3.5084782301630923e-15\n14582 8.918570837900507e-15\n14583 3.675228524291363e-15\n14584 7.50059702661534e-15\n14585 5.741154164622337e-15\n14586 5.732472500429454e-15\n14587 8.330611387893407e-15\n14588 4.33628818819593e-15\n14589 1.119440098343999e-14\n14590 3.761392950850807e-15\n14591 6.578648673857606e-15\n14592 4.506464730595844e-15\n14593 3.734035480720387e-15\n14594 2.6734826798804597e-15\n14595 2.9349922438839634e-15\n14596 2.495955373806036e-15\n14597 3.653384391614622e-15\n14598 4.13137397759617e-15\n14599 5.711937457172694e-15\n14600 3.841623911614734e-15\n14601 4.013166294642096e-15\n14602 3.850665988326674e-15\n14603 5.556181227183311e-15\n14604 3.6278408425407475e-15\n14605 8.418396188480948e-15\n14606 3.594097591020978e-15\n14607 7.64686945221079e-15\n14608 3.740120565413462e-15\n14609 9.215645621293324e-15\n14610 6.6930836720646095e-15\n14611 3.8385881455317745e-15\n14612 1.1553952069989337e-14\n14613 3.6987476646212995e-15\n14614 9.979007806984897e-15\n14615 8.00643493832024e-15\n14616 6.459470714662453e-15\n14617 1.0666478381701327e-14\n14618 4.68860264876041e-15\n14619 5.348100807426714e-15\n14620 4.764548046327704e-15\n14621 5.431777074703598e-15\n14622 3.3665777651248948e-15\n14623 7.332903984816775e-15\n14624 6.46983543332153e-15\n14625 5.74214349910473e-15\n14626 2.512131585516225e-15\n14627 4.7724330760336945e-15\n14628 5.345376749468344e-15\n14629 3.464690014921834e-15\n14630 4.242041373253786e-15\n14631 5.713910620423323e-15\n14632 3.636427639043538e-15\n14633 2.003922828591959e-15\n14634 3.059024972416305e-15\n14635 2.017917083430021e-15\n14636 3.0685523990070214e-15\n14637 2.8677282987513153e-15\n14638 2.044415026877214e-15\n14639 2.0588266572000354e-15\n14640 4.024948946454877e-15\n14641 2.962560836976486e-15\n14642 2.2429974544380116e-15\n14643 6.782259300751542e-15\n14644 5.455716343375373e-15\n14645 2.519729259294859e-15\n14646 3.793102899780692e-15\n14647 5.1151277834162075e-15\n14648 5.304884762941326e-15\n14649 2.985516065121788e-15\n14650 2.408633688482406e-15\n14651 4.426927489815718e-15\n14652 3.787717040785576e-15\n14653 2.4781094483985718e-15\n14654 3.4489906827609492e-15\n14655 4.6858349686052566e-15\n14656 4.123535111185805e-15\n14657 3.04287840685927e-15\n14658 2.6310253651575747e-15\n14659 3.4463019884281272e-15\n14660 2.378056434361236e-15\n14661 4.0220105891608515e-15\n14662 3.3577849278576847e-15\n14663 3.559869413138905e-15\n14664 3.3829384182593484e-15\n14665 4.1026997947492434e-15\n14666 5.952877249568601e-15\n14667 3.961252068821353e-15\n14668 3.5475014615595716e-15\n14669 4.688667446780875e-15\n14670 4.553345463127528e-15\n14671 4.201643830900386e-15\n14672 6.4522522998859515e-15\n14673 4.123516476460965e-15\n14674 2.7106058046180107e-15\n14675 3.299541671855058e-15\n14676 4.313519942573734e-15\n14677 3.238132418453832e-15\n14678 4.397949645657201e-15\n14679 2.874924690671188e-15\n14680 3.493006749865019e-15\n14681 2.4920956564236348e-15\n14682 3.8945977756524915e-15\n14683 2.465573360779208e-15\n14684 3.2198337539360604e-15\n14685 2.6905669109101053e-15\n14686 3.3907338742311666e-15\n14687 2.9307827018943463e-15\n14688 4.580634747105693e-15\n14689 3.524472965064332e-15\n14690 6.019525613506832e-15\n14691 4.537421243719147e-15\n14692 3.9990144916758445e-15\n14693 3.807449520324812e-15\n14694 7.145109157113971e-15\n14695 5.0194359302660474e-15\n14696 4.640203609670773e-15\n14697 4.0749658184737695e-15\n14698 7.519343559803972e-15\n14699 9.861243969295184e-15\n14700 1.2468509636765802e-14\n14701 9.913438139504994e-15\n14702 4.272363882216069e-15\n14703 4.405365419110413e-15\n14704 1.1753586724228861e-14\n14705 1.637827311382241e-14\n14706 1.1104737463775438e-14\n14707 4.512926321433974e-15\n14708 4.826426344223473e-15\n14709 1.3196095691864196e-14\n14710 1.4515786576213552e-14\n14711 9.807010143748386e-15\n14712 3.5307395265663562e-15\n14713 7.424890068822698e-15\n14714 1.0944948086407703e-14\n14715 1.0765314421151695e-14\n14716 5.7360368150715e-15\n14717 4.4504250308055e-15\n14718 7.2510678966178e-15\n14719 1.0147362381515267e-14\n14720 1.426962694328019e-14\n14721 5.456564646872048e-15\n14722 5.531696892809978e-15\n14723 1.5259191818634867e-14\n14724 1.6322501076443396e-14\n14725 1.3695565609871639e-14\n14726 6.521270238494126e-15\n14727 5.8803657235694754e-15\n14728 1.580695955529475e-14\n14729 1.3303594337266134e-14\n14730 9.642864475932923e-15\n14731 5.9897507113449485e-15\n14732 5.689513530443557e-15\n14733 1.0508878584502244e-14\n14734 3.6922640509265414e-15\n14735 5.544263050098969e-15\n14736 6.052710246797887e-15\n14737 5.5270809867639156e-15\n14738 3.5373965702570646e-15\n14739 2.3855835926470115e-15\n14740 3.84670610929826e-15\n14741 5.5632933393249314e-15\n14742 5.1935587991672194e-15\n14743 3.551296169163271e-15\n14744 3.84184879886223e-15\n14745 7.626422076864071e-15\n14746 1.2142717048329275e-14\n14747 1.5035963061649574e-14\n14748 1.5296542583476992e-14\n14749 1.5083767907126712e-14\n14750 1.0431773175313686e-14\n14751 7.656906792635754e-15\n14752 4.38081925133193e-15\n14753 3.379038043295479e-15\n14754 7.778283225845506e-15\n14755 1.0191861257399272e-14\n14756 6.892620070482252e-15\n14757 3.385887575223451e-15\n14758 3.126093676454611e-15\n14759 5.181065910228166e-15\n14760 3.2987609192359265e-15\n14761 3.263155889298092e-15\n14762 4.335475036566566e-15\n14763 4.51540939851885e-15\n14764 2.6092843587421618e-15\n14765 3.885357916747368e-15\n14766 4.202337974400661e-15\n14767 3.2739110901458535e-15\n14768 3.462543209917018e-15\n14769 4.603535976900608e-15\n14770 9.320443925659415e-15\n14771 1.3052059433249497e-14\n14772 1.0083202176859596e-14\n14773 6.2711663966418755e-15\n14774 3.25881908060815e-15\n14775 3.8419707716066346e-15\n14776 1.0334788749886015e-14\n14777 1.7206473130527457e-14\n14778 1.92118897017219e-14\n14779 1.3961195142130588e-14\n14780 7.700060580199519e-15\n14781 6.3750571052049836e-15\n14782 2.6547259822964605e-15\n14783 2.84316836668744e-15\n14784 6.666013769619783e-15\n14785 8.310856885497542e-15\n14786 8.798791061796329e-15\n14787 5.4385452914686334e-15\n14788 3.845982319644831e-15\n14789 5.142295094649968e-15\n14790 8.110233743808572e-15\n14791 1.2519119855364247e-14\n14792 1.1374408190225737e-14\n14793 1.1352217621073569e-14\n14794 1.1574432479620998e-14\n14795 1.0003470118501601e-14\n14796 5.471981917061497e-15\n14797 3.550805525328574e-15\n14798 4.734374615164138e-15\n14799 6.917245012324829e-15\n14800 9.516313518448394e-15\n14801 8.552588230182658e-15\n14802 7.45982340163336e-15\n14803 4.347859505288371e-15\n14804 1.082616018588476e-14\n14805 1.5944111130114166e-14\n14806 1.319275668798612e-14\n14807 1.0068522248850732e-14\n14808 5.217321884985965e-15\n14809 5.292503258834836e-15\n14810 3.4180259110664106e-15\n14811 9.435477776160127e-15\n14812 1.1927611337308151e-14\n14813 1.6478148462699058e-14\n14814 1.5548033441780372e-14\n14815 5.5635182265724274e-15\n14816 5.068376646925454e-15\n14817 3.222986198807504e-15\n14818 3.2740737204717263e-15\n14819 3.388435450328792e-15\n14820 3.6930636500287495e-15\n14821 3.4060562767305232e-15\n14822 4.811789614894918e-15\n14823 7.906576530102537e-15\n14824 8.484489322322254e-15\n14825 4.767757454164851e-15\n14826 2.947739666223667e-15\n14827 3.1234672390434122e-15\n14828 5.47248081946743e-15\n14829 9.257004546041857e-15\n14830 8.942773110302404e-15\n14831 3.515026853636552e-15\n14832 4.402221232810205e-15\n14833 6.11225496892397e-15\n14834 9.93919556439805e-15\n14835 1.2428588973961707e-14\n14836 1.50805017480821e-14\n14837 1.1454801781315537e-14\n14838 8.607838495266161e-15\n14839 5.8664578660920334e-15\n14840 4.5662394221671065e-15\n14841 3.896232125724219e-15\n14842 3.2955951335955636e-15\n14843 5.44331578102757e-15\n14844 3.587928650066125e-15\n14845 3.6592255308188874e-15\n14846 3.818942063353158e-15\n14847 4.577406704543707e-15\n14848 5.912037979533208e-15\n14849 3.596049154931452e-15\n14850 6.664384925262213e-15\n14851 5.115135830229206e-15\n14852 5.081178702890255e-15\n14853 1.5453436802231012e-14\n14854 1.5597102060414814e-14\n14855 9.045119254286194e-15\n14856 2.0034277378342888e-14\n14857 3.2491269061091926e-14\n14858 4.107802490630093e-14\n14859 5.4282695959723126e-14\n14860 7.120381893691366e-14\n14861 1.0346149663994358e-13\n14862 1.7781171771525522e-13\n14863 3.243376704362144e-13\n14864 5.709264441419726e-13\n14865 1.1546668569201168e-12\n14866 2.905451270199255e-12\n14867 8.534529853665429e-12\n14868 2.7749283773581546e-11\n14869 9.44431893801223e-11\n14870 3.322958574969448e-10\n14871 1.201007737705595e-09\n14872 4.40172431837027e-09\n14873 1.6243090428247342e-08\n14874 6.014997211423179e-08\n14875 2.223584090188524e-07\n14876 8.065522933975444e-07\n14877 2.7087430680694524e-06\n14878 7.002698112046346e-06\n14879 8.999828423839062e-06\n14880 2.196570903834072e-06\n14881 1.264315756088763e-06\n14882 5.706753199774539e-06\n14883 1.077248498404515e-06\n14884 1.8354810435994295e-06\n14885 3.4975043945451034e-06\n14886 2.5866626174320118e-08\n14887 2.7881637834070716e-06\n14888 9.58440978138242e-07\n14889 9.532692502034479e-07\n14890 1.8738058997769258e-06\n14891 5.5848335733799104e-08\n14892 1.7476997982157627e-06\n14893 1.961442421816173e-07\n14894 1.0060003887701896e-06\n14895 6.614399126192438e-07\n14896 3.2216453860201e-07\n14897 9.169554004984093e-07\n14898 1.903983992690428e-08\n14899 8.328328249262995e-07\n14900 5.3557329238174134e-08\n14901 5.420671413958189e-07\n14902 2.2723808967839432e-07\n14903 2.3955851702339714e-07\n14904 3.645495780801866e-07\n14905 5.156663718253185e-08\n14906 3.8681787373207044e-07\n14907 2.5671442749342077e-09\n14908 3.057680544316099e-07\n14909 4.588592972254446e-08\n14910 1.808450065254874e-07\n14911 1.1337245808817897e-07\n14912 7.252833711390849e-08\n14913 1.5419139742789412e-07\n14914 1.3801750853303929e-08\n14915 1.4999395148151962e-07\n14916 5.431204819217328e-09\n14917 1.1103549013569136e-07\n14918 2.687192512951242e-08\n14919 6.07893895221423e-08\n14920 5.261474456119686e-08\n14921 2.1058927046624376e-08\n14922 6.511233863193411e-08\n14923 2.256832942748588e-09\n14924 5.94907412221346e-08\n14925 2.697564838172184e-09\n14926 4.133688591423379e-08\n14927 1.3135760923432827e-08\n14928 2.070563098754974e-08\n14929 2.332430426577048e-08\n14930 5.895261168120669e-09\n14931 2.689144551482059e-08\n14932 2.120923853210499e-10\n14933 2.2997724613560422e-08\n14934 1.92449367553138e-09\n14935 1.4743635645686481e-08\n14936 6.671410091030339e-09\n14937 6.547451025795681e-09\n14938 1.0316298038048899e-08\n14939 1.4877715726058227e-09\n14940 1.0790768278923224e-08\n14941 2.958117084617129e-10\n14942 8.29908142208069e-09\n14943 1.7179647704423928e-09\n14944 4.607284331825667e-09\n14945 3.7167022792772286e-09\n14946 1.5705456934966833e-09\n14947 4.744454162874945e-09\n14948 1.6227195975826447e-10\n14949 4.29210711416772e-09\n14950 2.995897696589367e-10\n14951 2.8256021966655e-09\n14952 1.185890718957694e-09\n14953 1.2434099305735913e-09\n14954 1.9362276226786435e-09\n14955 2.373915231057566e-10\n14956 2.0531432110004744e-09\n14957 9.684495393125214e-12\n14958 1.5781813633708452e-09\n14959 3.1338903694333453e-10\n14960 8.46760883632669e-10\n14961 7.258847967150928e-10\n14962 2.589871095359797e-10\n14963 9.101420173074359e-10\n14964 2.2963556403632346e-11\n14965 7.908944610335311e-10\n14966 9.344643808750774e-11\n14967 4.769394879033939e-10\n14968 2.812751420666615e-10\n14969 1.7917596573102657e-10\n14970 4.0966305348000276e-10\n14971 2.2458281762061105e-11\n14972 3.88025778175205e-10\n14973 2.8456995440628852e-11\n14974 2.529502718395804e-10\n14975 1.1629069929242064e-10\n14976 1.0270782835331005e-10\n14977 1.8788193223429062e-10\n14978 1.3499085529944388e-11\n14979 1.9144627549927407e-10\n14980 5.729791207470525e-12\n14981 1.3252117458950607e-10\n14982 4.535283606998952e-11\n14983 5.977979222748786e-11\n14984 8.544080720707115e-11\n14985 1.0446980953926488e-11\n14986 9.154656199772404e-11\n14987 2.0141095822406863e-12\n14988 6.619787357164952e-11\n14989 1.9543529988785835e-11\n14990 3.071388629538596e-11\n14991 3.859719141519058e-11\n14992 6.381607048355775e-12\n14993 4.2488897816772564e-11\n14994 1.6735453084101848e-12\n14995 3.0845291598691205e-11\n14996 9.912176981985432e-12\n14997 1.3890902317292841e-11\n14998 1.90799129967667e-11\n14999 2.369806174212785e-12\n15000 2.0259493735408363e-11\nloss =  0.015542561188340187\n1 tensor([0.2112, 0.7995], grad_fn=<SelectBackward>) [0, 1] [tensor(0), tensor(1)]\n2 tensor([0.0420, 0.9276], grad_fn=<SelectBackward>) [0, 1] [tensor(0), tensor(1)]\n3 tensor([-0.1134,  1.0985], grad_fn=<SelectBackward>) [0, 1] [tensor(0), tensor(1)]\n4 tensor([-0.1546,  1.2086], grad_fn=<SelectBackward>) [0, 1] [tensor(0), tensor(1)]\n5 tensor([-0.0037,  0.9906], grad_fn=<SelectBackward>) [0, 1] [tensor(0), tensor(1)]\n6 tensor([0.1152, 0.9525], grad_fn=<SelectBackward>) [0, 1] [tensor(0), tensor(1)]\n7 tensor([0.1753, 0.7754], grad_fn=<SelectBackward>) [0, 1] [tensor(0), tensor(1)]\n8 tensor([0.0329, 1.0299], grad_fn=<SelectBackward>) [0, 1] [tensor(0), tensor(1)]\n9 tensor([-0.0319,  0.9873], grad_fn=<SelectBackward>) [0, 1] [tensor(0), tensor(1)]\n10 tensor([-0.0303,  1.0375], grad_fn=<SelectBackward>) [0, 1] [tensor(0), tensor(1)]\n11 tensor([-0.0403,  1.0066], grad_fn=<SelectBackward>) [0, 1] [tensor(0), tensor(1)]\n12 tensor([0.2528, 0.7644], grad_fn=<SelectBackward>) [0, 1] [tensor(0), tensor(1)]\n13 tensor([-0.0267,  0.9799], grad_fn=<SelectBackward>) [0, 1] [tensor(0), tensor(1)]\n14 tensor([0.9465, 0.0307], grad_fn=<SelectBackward>) [1, 0] [tensor(1), tensor(0)]\n15 tensor([ 0.9520, -0.1153], grad_fn=<SelectBackward>) [1, 0] [tensor(1), tensor(0)]\n16 tensor([0.7712, 0.3087], grad_fn=<SelectBackward>) [1, 0] [tensor(1), tensor(0)]\n17 tensor([1.0290, 0.0088], grad_fn=<SelectBackward>) [1, 0] [tensor(1), tensor(0)]\n18 tensor([ 9.5311e-01, -6.1733e-04], grad_fn=<SelectBackward>) [1, 0] [tensor(1), tensor(0)]\n19 tensor([0.9643, 0.0524], grad_fn=<SelectBackward>) [1, 0] [tensor(1), tensor(0)]\n20 tensor([0.9242, 0.0171], grad_fn=<SelectBackward>) [1, 0] [tensor(1), tensor(0)]\n21 tensor([ 1.1775, -0.1866], grad_fn=<SelectBackward>) [1, 0] [tensor(1), tensor(0)]\n"
    }
   ],
   "source": [
    "for j in [10, 20, 50, 100, 200, 300, 400, 500, 1000, 2000, 5000, 10000, 15000]:\n",
    "    for i in range(j):\n",
    "        predy = model(indata)\n",
    "\n",
    "        loss = lossfunc(predy, outdata)\n",
    "        print(i+1, loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "    check(model, din, dout)"
   ]
  },
  {
   "source": [
    "import time\n",
    "tstart = time.time()\n",
    "testdata = []\n",
    "testdata_norm = []\n",
    "\n",
    "for i in range(10):\n",
    "    testdata.append(get_data(r'D:\\\\My Files\\\\核技术专题研讨\\\\Anode\\\\Anode\\\\c3--0000' + str(i) + '.csv'))\n",
    "for i in range(90):\n",
    "    testdata.append(get_data(r'D:\\\\My Files\\\\核技术专题研讨\\\\Anode\\\\Anode\\\\c3--000' + str(i+10) + '.csv'))\n",
    "for i in range(900):\n",
    "    testdata.append(get_data(r'D:\\\\My Files\\\\核技术专题研讨\\\\Anode\\\\Anode\\\\c3--00' + str(i+100) + '.csv'))\n",
    "for i in range(9000):\n",
    "    testdata.append(get_data(r'D:\\\\My Files\\\\核技术专题研讨\\\\Anode\\\\Anode\\\\c3--0' + str(i+1000) + '.csv'))\n",
    "for i in range(9243):\n",
    "    testdata.append(get_data(r'D:\\\\My Files\\\\核技术专题研讨\\\\Anode\\\\Anode\\\\c3--' + str(i+10000) + '.csv'))\n",
    "\n",
    "for i in range(len(testdata)):\n",
    "    testdata_norm.append(normalize(testdata[i]))\n",
    "    writedata(testdata_norm[i], str(i) + '.txt')\n",
    "\n",
    "testdata = torch.tensor(testdata)\n",
    "tend = time.time()\n",
    "print(tend - tstart)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "41.18225455284119\n"
     ]
    }
   ]
  },
  {
   "source": [
    "result = []\n",
    "gamma = 0\n",
    "neutron = 0\n",
    "output = model(testdata)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Data processing and output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "for i in range(len(testdata)):\n",
    "    a = output[i][0].item()\n",
    "    b = output[i][1].item()\n",
    "    if a > b:\n",
    "        result.append([1, 0])\n",
    "        par = 'gamma'\n",
    "        gamma += 1\n",
    "    elif b > a:\n",
    "        result.append([0, 1])\n",
    "        par = 'neutron'\n",
    "        neutron += 1\n",
    "    with open('result.txt', 'a') as f:\n",
    "        #print(i+1, a, b, result[i])\n",
    "        print('file = ' + str(i), 'particle =', par, file=f)\n",
    "    with open('stat.txt', 'a') as f:\n",
    "        print(b, file=f)\n",
    "with open('result.txt', 'a') as f:\n",
    "    print('total gamma =', gamma, 'total neutron =', neutron, file=f)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(signal):\n",
    "    peak = min(signal)\n",
    "    norm = signal\n",
    "    for i in range(len(signal)):\n",
    "        norm[i] = signal[i]/peak\n",
    "    return norm\n",
    "\n",
    "def writedata(signal, filename):\n",
    "    with open('norm/' + filename, 'a') as f:\n",
    "        for i in range(len(signal)):\n",
    "            print(signal[i], file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for i in range(192):\n",
    "    shutil.copyfile('norm/' + str(100*i) + '.txt', 'sample/' + str(10*i) + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}